koko login: asturm2017@koko-login.hpc.fau.edu

###FK MCAV Reads Processing Protocol

##Downloading Reads from Basespace
#Go to the bin on your local drive

cd ~/bin
brew tap basespace/basespace && brew install bs-cli #Use homebrew to install the basespace command line downloader
bs auth #Should generate a link to authenticate the usage, go to the link in your browser
bs download project --name JA20179 -o asturm2017@koko-login.hpc.fau.edu:~/2bRAD/floridaKeys/rawReads #--name is the project name and -o is the output directory, in this case uploading the reads directly to koko

# bs download project --name JA20179 -o /Users/student/Documents/Florida\ Keys/rawReads #Or download to s home directory

cd ~/2bRAD/floridaKeys/rawReads #There should be 50 files

################################################################################

mkdir concatReads #Make a new directory where you can concatenate your reads between the two lanes.
srun cp ./rawReads/*.fastq.gz ./concatReads
cd concatReads
gunzip *.fastq.gz #unzip all of your files

nano concat.sh #See script below, concatenate all of your files across lanes

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu

cat 10_S38_L001_R1_001.fastq 10_S38_L002_R1_001.fastq > fk10.fq
cat 11_S39_L001_R1_001.fastq 11_S39_L002_R1_001.fastq > fk11.fq
cat 12_S40_L001_R1_001.fastq 12_S40_L002_R1_001.fastq > fk12.fq
cat 13_S41_L001_R1_001.fastq 13_S41_L002_R1_001.fastq > fk13.fq
cat 14_S42_L001_R1_001.fastq 14_S42_L002_R1_001.fastq > fk14.fq
cat 15_S43_L001_R1_001.fastq 15_S43_L002_R1_001.fastq > fk15.fq
cat 16_S44_L001_R1_001.fastq 16_S44_L002_R1_001.fastq > fk16.fq
cat 17_S45_L001_R1_001.fastq 17_S45_L002_R1_001.fastq > fk17.fq
cat 18_S46_L001_R1_001.fastq 18_S46_L002_R1_001.fastq > fk18.fq
cat 19_S47_L001_R1_001.fastq 19_S47_L002_R1_001.fastq > fk19.fq
cat 1_S29_L001_R1_001.fastq 1_S29_L002_R1_001.fastq > fk1.fq
cat 20_S48_L001_R1_001.fastq 20_S48_L002_R1_001.fastq > fk20.fq
cat 21_S49_L001_R1_001.fastq 21_S49_L002_R1_001.fastq > fk21.fq
cat 22_S50_L001_R1_001.fastq 22_S50_L002_R1_001.fastq > fk22.fq
cat 23_S51_L001_R1_001.fastq 23_S51_L002_R1_001.fastq > fk23.fq
cat 24_S52_L001_R1_001.fastq 24_S52_L002_R1_001.fastq > fk24.fq
cat 25_S53_L001_R1_001.fastq 25_S53_L002_R1_001.fastq > fk25.fq
cat 2_S30_L001_R1_001.fastq 2_S30_L002_R1_001.fastq > fk2.fq
cat 3_S31_L001_R1_001.fastq 3_S31_L002_R1_001.fastq > fk3.fq
cat 4_S32_L001_R1_001.fastq 4_S32_L002_R1_001.fastq > fk4.fq
cat 5_S33_L001_R1_001.fastq 5_S33_L002_R1_001.fastq > fk5.fq
cat 6_S34_L001_R1_001.fastq 6_S34_L002_R1_001.fastq > fk6.fq
cat 7_S35_L001_R1_001.fastq 7_S35_L002_R1_001.fastq > fk7.fq
cat 8_S36_L001_R1_001.fastq 8_S36_L002_R1_001.fastq > fk8.fq
cat 9_S37_L001_R1_001.fastq 9_S37_L002_R1_001.fastq > fk9.fq

################################################################################
mkdir trimmedReads
srun cp ./concatReads/*.fq ./trimmedReads

#Set up a script to trim and deduplicate the files
2bRAD_trim_launch_dedup.pl fq > trims #Trim and deduplicate files
launcher_creator.py -j trims -n trims -t 2:00:00 -e asturm2017@fau.edu -q shortq7
sbatch trims.slurm

################################################################################
mkdir renamedReads
srun cp ./trimmedReads/*.tr0 ./renamedReads

#Create and scp a csv file with a column of file names (as deduplicated with the in-line barcodes) and what you want to rename the filed to (sample ID #). Upload the csv to the working directory and copy the sampleRename.py script

python sampleRename.py -i sampleRename -f tr0

################################################################################
mkdir highQualityReads
srun cp ./renamedReads/*.tr0 ./highQualityReads

# Quality filtering using fastx_toolkit
# Creating a list of filtering commands:
# The options -q 20 -p 90 mean that 90% or more of all bases within the read should have PHRED quality of at least 20 (i.e., probability of error 1% or less)
# PHRED quality=10*(-log10(P.error))
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastq_quality_filter -q 20 -p 90 >$1\.trim/' >filt0

# NOTE: run the next line ONLY if your qualities are 33-based (GSAF results are 33-based):
cat filt0 | perl -pe 's/filter /filter -Q33 /' > filt

################################################################################
mkdir mappedReadsNoConcat
srun cp ./highQualityReads/*.trim ./mappedReadsNoConcat

mkdir referenceGenome #Ensure that the concatenated MCAV and algal symbiont transcriptomes are in this directory along with the associated genome index files

cd mappedReadsNoConcat

GENOME_FASTA=/home/asturm2017/2bRAD/floridaKeys/referenceGenome/mcav_syms_updated.fasta

# mapping with --local option, enables clipping of mismatching ends (guards against deletions near ends of RAD tags)
2bRAD_bowtie2_launch.pl '\.trim$' $GENOME_FASTA > maps #Execute all commands in maps

#Check to ensure that the number of sams files made matches the number of bams files

################################################################################
mkdir bamsNoConcat
srun cp ./mappedReadsNoConcat/*.sam ./bamsNoConcat

>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

#Execute all commands written to s2b
#Ensure that the number of output bams files matches the number of original sams files

###Now we do ANGSD for the purpose of producing an IBS matrix so we can generate an IBS dendrogram and check to make sure all re-extracts, technical replicates, etc. are clones of one another

export GENOME_REF=/home/asturm2017/2bRAD/floridaKeys/referenceGenome/Mcavernosa_July2018.fasta


FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -baq 1 -ref $GENOME_REF -maxDepth 25700"
TODO="-doQsDist 1 -doDepth 1 -doCounts 1"
angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out dd


Rscript ~/bin/plotQC.R dd
cat dd.info
# scp dd.pdf to laptop to look at distribution of base quality scores, fraction of sites in each sample passing coverage thresholds, and fraction of sites passing genotyping rates cutoffs. Use these to guide choices of -minQ,  -minIndDepth and -minInd filters in subsequent ANGSD runs

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 30 -minInd 205 -snp_pval 1e-5 -minMaf 0.05 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5"
TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 32 -doPost 1 -doGlf 2"
angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out noSamplesConcat

NSITES=`zcat noSamplesConcat.beagle.gz | wc -l`
echo $NSITES
## 13,819 SNPs

#scp the ibsMAT and use the output to generate a cluster dendrogram so you can identify, repeated samples, clones, etc.

################################################################################
mkdir tr0Concat
srun cp ./renamedReads/*.tr0 ./tr0Concat
###Concatenate any files that may be re-extracts or bad barcode files that are clones of good bc samples.
##For now leave technical replicates and natural clones

#YOU SHOULD HAVE 223 FILES

################################################################################
mkdir highQualityConcat
srun cp ./tr0Concat/*.tr0 ./highQualityConcat

# Quality filtering using fastx_toolkit
# The options -q 20 -p 90 mean that 90% or more of all bases within the read should have PHRED quality of at least 20 (i.e., probability of error 1% or less)
# PHRED quality=10*(-log10(P.error))
ls *.tr0 | perl -pe 's/^(\S+)\.tr0$/cat $1\.tr0 \| fastq_quality_filter -q 20 -p 90 >$1\.trim/' >filt0

# NOTE: run the next line ONLY if your qualities are 33-based (GSAF results are 33-based):
cat filt0 | perl -pe 's/filter /filter -Q33 /' > filt
#if you did NOT run the line above, run this one:
mv filt0 filt

launcher_creator.py -j filt -n filt -t 1:00:00 -e asturm2017@fau.edu -q shortq7
sbatch filt.slurm

################################################################################
mkdir samsConcat
srun cp ./highQualityConcat/*.trim ./samsConcat

# Mapping reads to a reference genome with soft-clipping (Bowtie2 --local option) to avoid indels near read ends
GGENOME_FASTA=/home/asturm2017/2bRAD/floridaKeys/referenceGenome/mcav_syms_updated.fasta
2bRAD_bowtie2_launch.pl '\.trim$' $GENOME_FASTA > maps
launcher_creator.py -j maps -n maps -t 2:00:00 -e asturm2017@fau.edu -q shortq7
sbatch maps.slurm

#See how many reads aligned with zoox transcriptomes
#Using ~2Mb contig as a host reference; look up contig lengths in the header of any sam file
#Run scripts to count read alignments to each of the algal symbiont transcriptomes

zooxType.pl host="Sc0000000" >zooxCounts.txt

#scp zooxCounts.txt to local directory and open as csv file

################################################################################
mkdir mcavSams
srun cp ./samsConcat/*.sam ./mcavSams

#Re-write sams files without any reads aligning to algal symbiont transcriptomes

for f in *.sam
do
  egrep -v "chr11|chr12|chr13|chr14" < "$f" > "$f".mcav.sam
done

#These sam files should now only have MCAV reads

################################################################################
mkdir zooxSams
srun cp ./samsConcat/*.sam ./zooxSams

#Re-write sams files without any reads aligning to algal symbiont transcriptomes

for f in *.trim.bt2.sam
do
  egrep "chr11|chr12|chr13|chr14" < "$f" > "$f".trim.bt2.zoox.sam
done

#These sam files should now only have zoox reads

################################################################################
mkdir mcavBams
srun cp ./mcavSams/*.sam ./mcavBams

#Compressing, sorting and indexing the SAM files, so they become BAM files:
>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

launcher_creator.py -j s2b -n s2b -t 1:00:00 -N 5 -e asturm2017@fau.edu -q shortq7
sbatch s2b.slurm

ls *bam >bams

#Archive these BAMS files

#Read counts
for i in *.bam; do
    echo $i >> output
    samtools view $i | cut -f1 | sort | uniq | wc -l >> output
done

################################################################################
mkdir zooxBams
srun cp ./zooxSams/*.sam ./zooxBams

#Compressing, sorting and indexing the SAM files, so they become BAM files:
>s2b
for file in *.sam; do
echo "samtools sort -O bam -o ${file/.sam/}.bam $file && samtools index ${file/.sam/}.bam">>s2b;
done

launcher_creator.py -j s2b -n s2b -t 1:00:00 -N 5 -e asturm2017@fau.edu -q shortq7
sbatch s2b.slurm

ls *bam >bams

#Archive these BAMS files

#Read counts
for i in *.bam; do
    echo $i >> output
    samtools view $i | cut -f1 | sort | uniq | wc -l >> output
done

################################################################################
mkdir mcavANGSDClones
srun cp ./mcavBams/*bam* ./mcavANGSDClones/

# angsd settings:
# -minMapQ 20 : only highly unique mappings (prob of erroneous mapping = 1%)
# -baq 1 : realign around indels (not terribly relevant for 2bRAD reads mapped with --local option)
# -maxDepth : highest total depth (sum over all samples) to assess; set to 10x number of samples

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -maxDepth 22300"

# T O   D O :

TODO="-doQsDist 1 -doDepth 1 -doCounts 1 -dumpCounts 2"

srun angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out ddClones

# summarizing results (using modified script by Matteo Fumagalli)

Rscript ~/bin/plotQC.R ddClones > qranks

# proportion of sites covered at >5x:

cat qranks

# scp dd.pdf to laptop to look at distribution of base quality scores, fraction of sites in each sample passing coverage thresholds, and fraction of sites passing genotyping rates cutoffs. Use these to guide choices of -minQ,  -minIndDepth and -minInd filters in subsequent ANGSD runs

##ANGSD WITH NEW FILTERS W CLONES

# Note: PCA and Admixture are not supposed to be run on data that contain clones or genotyping replicates. For PCA, these can be removed without rerunning ANGSD from the IBS distance matrix; but for ngsAdmix ANGSD must be rerun.

# Generating genotype likelihoods from highly confident (non-sequencing-error) SNPs
# set minInd to 75-80% of your total number of bams
# if you expect very highly differentiated populations with nearly fixed alternative alleles, remove '-hwe_pval 1e-5' form FILTERS
# -doGeno 8 : genotype likelihood format setting for ngsLD; if you want to run PCA, use -doGeno 32 (but I recommend using ibsMat for all ordination work)

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 168 -snp_pval 1e-5 -minMaf 0.05"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doVcf 1 -doPost 1 -doGlf 2"

# Starting angsd with -P the number of parallel processes. Funny but in many cases angsd runs faster on -P 1

srun angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out fkMcavClones

# how many SNPs?
NSITES=`zcat fkMcavClones.mafs.gz | wc -l`
echo $NSITES

#scp the ibs matrix to identify clones

################################################################################
mkdir mcavBamsNoClones
srun cp ./mcavBams/*.bam* ./mcavBamsNoClone

#Manually remove clones and technical replicates based on IBS dendrogram
#YOU SHOULD HAVE 215 FILES

################################################################################
mkdir mcavANGSDNoClones
srun cp ./mcavBamsNoClone/*bam* ./mcavANGSDNoClones

# Note: PCA and Admixture are not supposed to be run on data that contain clones or genotyping replicates. For PCA, these can be removed without rerunning ANGSD from the IBS distance matrix; but for ngsAdmix ANGSD must be rerun.

# Generating genotype likelihoods from highly confident (non-sequencing-error) SNPs
# set minInd to 75-80% of your total number of bams
# if you expect very highly differentiated populations with nearly fixed alternative alleles, remove '-hwe_pval 1e-5' form FILTERS
# -doGeno 8 : genotype likelihood format setting for ngsLD; if you want to run PCA, use -doGeno 32 (but I recommend using ibsMat for all ordination work)

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -dosnpstat 1 -doHWE 1 -hwe_pval 1e-5 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 162 -snp_pval 1e-5 -minMaf 0.05"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doVcf 1 -doPost 1 -doGlf 2"

# Starting angsd with -P the number of parallel processes. Funny but in many cases angsd runs faster on -P 1

srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out fkMcavNoClones

# how many SNPs?
NSITES=`zcat fkMcavNoClones.mafs.gz | wc -l`
echo $NSITES

# NgsAdmix for K from 2 to 11 : do not run if the dataset contains clones or genotyping replicates!
for K in `seq 2 11` ;
do
NGSadmix -likes fkMcavNoClones.beagle.gz -K $K -P 10 -o fkMcavNoClones_k${K};
done

# alternatively, to use real ADMIXTURE on called SNPs (requires plink and ADMIXTURE):
gunzip fkMcavNoClones.vcf.gz
cat fkMcavNoClones.vcf | sed 's/xpSc//g' >fkMcavNoClones_chr.vcf
cat fkMcavNoClones_chr.vcf | sed 's/xfSc//g' >fkMcavNoClones_chr1.vcf
cat fkMcavNoClones_chr1.vcf | sed 's/Sc//g' >fkMcavNoClones_chr2.vcf
plink --vcf fkMcavNoClones_chr2.vcf --make-bed --allow-extra-chr --out fkMcavNoClones
for K in `seq 1 11`; \
do admixture --cv fkMcavNoClones.bed $K | tee fkMcavNoClones_${K}.out; done

grep -h CV fkMcavNoClones*.out

####
CV error (K=1): 0.48446
CV error (K=2): 0.43832
CV error (K=3): 0.43216
CV error (K=4): 0.43160
CV error (K=5): 0.43222
CV error (K=6): 0.43130
CV error (K=7): 0.43355
CV error (K=8): 0.44174
CV error (K=9): 0.45271
CV error (K=10): 0.46330
CV error (K=11): 0.46838

bcftools reheader fkMcavNoClones.vcf.gz -s sampleListNoClones -o fkMcavNoClonesRenamed.vcf.gz

# LD: (use rEM for WGCNA, to look for signatures of polygenic selection):
NS=`zcat fkMcavNoClones.geno.gz | wc -l`
NB=`cat bamsNoClones | wc -l`
zcat fkMcavNoClones.mafs.gz | tail -n +2 | cut -f 1,2 > mc1.sites
module load gsl
ngsLD --geno fkMcavNoClones.geno.gz --probs 1 --n_ind $NB --n_sites $NS --max_kb_dist 0 --pos mc1.sites --out fkMcavNoClones.LD --n_threads 12 --extend_out 1

# individual heterozygosities (proportion of heterozygotes across SNPs that pass filters)
Rscript ~/bin/heterozygosity_beagle.R fkMcavNoClones.beagle.gz
#Prints individual heterozygosity

#Relatedness
FILTERS="-uniqueOnly 1 -remove_bads 1 -skipTriallelic 1 -minMapQ 20 -minQ 25 -dosnpstat 1 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -minInd 162 -snp_pval 1e-5 -minMaf 0.05"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGlf 3"

# Starting angsd with -P the number of parallel processes. Funny but in many cases angsd runs faster on -P 1

srun angsd -b bamsNoClones -GL 1 $FILTERS $TODO -P 1 -out fkMcavNoClonesRelated

zcat fkMcavNoClonesRelated.mafs.gz | cut -f5 |sed 1d >freq
NIND=`cat bamsNoClones | wc -l`
srun ngsRelate -f freq -g fkMcavNoClonesRelated.glf.gz -n $NIND -z bamsNoClones -O relatednessValues


##SFS Analysis
FILTERS="-uniqueOnly 1 -remove_bads 1  -skipTriallelic 1 -minMapQ 20 -minQ 25 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -minInd 162 "
TODO="-doMajorMinor 1 -doMaf 1 -dosnpstat 1 -doPost 2 -doGeno 11"
# ANGSD command:
angsd -b bamsNoClones -GL 1 -P 1 $FILTERS $TODO -out sfilt

###HETMAJORITYPROB DID NOT WORK
# extracting and indexing list of sites to make SFS from
# filtering out sites where heterozygote counts comprise more than 50% of all counts (likely lumped paralogs)
zcat sfilt.snpStat.gz | awk '($3+$4+$5+$6)>0' | awk '($12+$13+$14+$15)/($3+$4+$5+$6)<0.5' | cut -f 1,2  >sites2do
angsd sites index sites2do

#pop0=Lower Keys-Meso, 13
#pop1=Lower Keys-shallow, 30
#pop2=TER-North-Meso, 23
#pop3=TER-North-shallow, 27
#pop4=TER-South-Meso, 37
#pop5=TER-South-shallow, 28
#pop6=Upper-Keys-Meso, 25
#pop7=Upper-Keys-Shallow, 32


# estimating site frequency likelihoods for each population, also saving allele frequencies (for genome scan)
export GENOME_REF=/home/asturm2017/2bRAD/floridaKeys/referenceGenome/Mcavernosa_July2018.fasta
TODO="-doSaf 1 -doMajorMinor 1 -doMaf 1 -doPost 1 -anc $GENOME_REF -ref $GENOME_REF"

nano sfiltAngsd
#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu
srun angsd -sites sites2do -b pop0.bams -minInd 10 -GL 1 -P 1 $TODO -out pop0
srun angsd -sites sites2do -b pop1.bams -minInd 23 -GL 1 -P 1 $TODO -out pop1
srun angsd -sites sites2do -b pop2.bams -minInd 18 -GL 1 -P 1 $TODO -out pop2
srun angsd -sites sites2do -b pop3.bams -minInd 21 -GL 1 -P 1 $TODO -out pop3
srun angsd -sites sites2do -b pop4.bams -minInd 28 -GL 1 -P 1 $TODO -out pop4
srun angsd -sites sites2do -b pop5.bams -minInd 21 -GL 1 -P 1 $TODO -out pop5
srun angsd -sites sites2do -b pop6.bams -minInd 19 -GL 1 -P 1 $TODO -out pop6
srun angsd -sites sites2do -b pop7.bams -minInd 24 -GL 1 -P 1 $TODO -out pop7

# generating per-population SFS
srun realSFS pop0.saf.idx >pop0.sfs
srun realSFS pop1.saf.idx >pop1.sfs
srun realSFS pop2.saf.idx >pop2.sfs
srun realSFS pop3.saf.idx >pop3.sfs
srun realSFS pop4.saf.idx >pop4.sfs
srun realSFS pop5.saf.idx >pop5.sfs
srun realSFS pop6.saf.idx >pop6.sfs
srun realSFS pop7.saf.idx >pop7.sfs


# generating dadi-like posterior counts based on sfs priors
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout

# converting to dadi-snp format understood by dadi an Moments:
# (numbers after the input file name are numbers of individuals sampled per population)
realsfs2dadi.pl dadiout 20 20 >2pops_dadi.data

##Could not get the realSFS subfunction to work on KOKO
#downloaded and installed angsd on local drive and scp SAF_analysis directory into local angsd directory

nano makeml.sh
#Add the information between the carrots
>>>>>>>
#!/bin/bash
../misc/realSFS pop1.saf.idx pop2.saf.idx -P 24 >pop1.pop2.ml
../misc/realSFS pop1.saf.idx pop3.saf.idx -P 24 >pop1.pop3.ml
../misc/realSFS pop1.saf.idx pop4.saf.idx -P 24 >pop1.pop4.ml
../misc/realSFS pop1.saf.idx pop5.saf.idx -P 24 >pop1.pop5.ml
../misc/realSFS pop1.saf.idx pop6.saf.idx -P 24 >pop1.pop6.ml
../misc/realSFS pop1.saf.idx pop7.saf.idx -P 24 >pop1.pop7.ml
../misc/realSFS pop1.saf.idx pop8.saf.idx -P 24 >pop1.pop8.ml
../misc/realSFS pop2.saf.idx pop3.saf.idx -P 24 >pop2.pop3.ml
../misc/realSFS pop2.saf.idx pop4.saf.idx -P 24 >pop2.pop4.ml
../misc/realSFS pop2.saf.idx pop5.saf.idx -P 24 >pop2.pop5.ml
../misc/realSFS pop2.saf.idx pop6.saf.idx -P 24 >pop2.pop6.ml
../misc/realSFS pop2.saf.idx pop7.saf.idx -P 24 >pop2.pop7.ml
../misc/realSFS pop2.saf.idx pop8.saf.idx -P 24 >pop2.pop8.ml
../misc/realSFS pop3.saf.idx pop4.saf.idx -P 24 >pop3.pop4.ml
../misc/realSFS pop3.saf.idx pop5.saf.idx -P 24 >pop3.pop5.ml
../misc/realSFS pop3.saf.idx pop6.saf.idx -P 24 >pop3.pop6.ml
../misc/realSFS pop3.saf.idx pop7.saf.idx -P 24 >pop3.pop7.ml
../misc/realSFS pop3.saf.idx pop8.saf.idx -P 24 >pop3.pop8.ml
../misc/realSFS pop4.saf.idx pop5.saf.idx -P 24 >pop4.pop5.ml
../misc/realSFS pop4.saf.idx pop6.saf.idx -P 24 >pop4.pop6.ml
../misc/realSFS pop4.saf.idx pop7.saf.idx -P 24 >pop4.pop7.ml
../misc/realSFS pop4.saf.idx pop8.saf.idx -P 24 >pop4.pop8.ml
../misc/realSFS pop5.saf.idx pop6.saf.idx -P 24 >pop5.pop6.ml
../misc/realSFS pop5.saf.idx pop7.saf.idx -P 24 >pop5.pop7.ml
../misc/realSFS pop5.saf.idx pop8.saf.idx -P 24 >pop5.pop8.ml
../misc/realSFS pop6.saf.idx pop7.saf.idx -P 24 >pop6.pop7.ml
../misc/realSFS pop6.saf.idx pop8.saf.idx -P 24 >pop6.pop8.ml
../misc/realSFS pop7.saf.idx pop8.saf.idx -P 24 >pop7.pop8.ml
>>>>>>

chmod +x makeml.sh
./makeml.sh

nano fst.sh #between the carrots put:
>>>>>>
#!/bin/bash
../misc/realSFS fst index pop0.saf.idx pop1.saf.idx -sfs pop0.pop1.ml -fstout pop0.pop1
../misc/realSFS fst index pop0.saf.idx pop2.saf.idx -sfs pop0.pop2.ml -fstout pop0.pop2
../misc/realSFS fst index pop0.saf.idx pop3.saf.idx -sfs pop0.pop3.ml -fstout pop0.pop3
../misc/realSFS fst index pop0.saf.idx pop4.saf.idx -sfs pop0.pop4.ml -fstout pop0.pop4
../misc/realSFS fst index pop0.saf.idx pop5.saf.idx -sfs pop0.pop5.ml -fstout pop0.pop5
../misc/realSFS fst index pop0.saf.idx pop6.saf.idx -sfs pop0.pop6.ml -fstout pop0.pop6
../misc/realSFS fst index pop0.saf.idx pop7.saf.idx -sfs pop0.pop7.ml -fstout pop0.pop7
../misc/realSFS fst index pop1.saf.idx pop2.saf.idx -sfs pop1.pop2.ml -fstout pop1.pop2
../misc/realSFS fst index pop1.saf.idx pop3.saf.idx -sfs pop1.pop3.ml -fstout pop1.pop3
../misc/realSFS fst index pop1.saf.idx pop4.saf.idx -sfs pop1.pop4.ml -fstout pop1.pop4
../misc/realSFS fst index pop1.saf.idx pop5.saf.idx -sfs pop1.pop5.ml -fstout pop1.pop5
../misc/realSFS fst index pop1.saf.idx pop6.saf.idx -sfs pop1.pop6.ml -fstout pop1.pop6
../misc/realSFS fst index pop1.saf.idx pop7.saf.idx -sfs pop1.pop7.ml -fstout pop1.pop7
../misc/realSFS fst index pop2.saf.idx pop3.saf.idx -sfs pop2.pop3.ml -fstout pop2.pop3
../misc/realSFS fst index pop2.saf.idx pop4.saf.idx -sfs pop2.pop4.ml -fstout pop2.pop4
../misc/realSFS fst index pop2.saf.idx pop5.saf.idx -sfs pop2.pop5.ml -fstout pop2.pop5
../misc/realSFS fst index pop2.saf.idx pop6.saf.idx -sfs pop2.pop6.ml -fstout pop2.pop6
../misc/realSFS fst index pop2.saf.idx pop7.saf.idx -sfs pop2.pop7.ml -fstout pop2.pop7
../misc/realSFS fst index pop3.saf.idx pop4.saf.idx -sfs pop3.pop4.ml -fstout pop3.pop4
../misc/realSFS fst index pop3.saf.idx pop5.saf.idx -sfs pop3.pop5.ml -fstout pop3.pop5
../misc/realSFS fst index pop3.saf.idx pop6.saf.idx -sfs pop3.pop6.ml -fstout pop3.pop6
../misc/realSFS fst index pop3.saf.idx pop7.saf.idx -sfs pop3.pop7.ml -fstout pop3.pop7
../misc/realSFS fst index pop4.saf.idx pop5.saf.idx -sfs pop4.pop5.ml -fstout pop4.pop5
../misc/realSFS fst index pop4.saf.idx pop6.saf.idx -sfs pop4.pop6.ml -fstout pop4.pop6
../misc/realSFS fst index pop4.saf.idx pop7.saf.idx -sfs pop4.pop7.ml -fstout pop4.pop7
../misc/realSFS fst index pop5.saf.idx pop6.saf.idx -sfs pop5.pop6.ml -fstout pop5.pop6
../misc/realSFS fst index pop5.saf.idx pop7.saf.idx -sfs pop5.pop7.ml -fstout pop5.pop7
../misc/realSFS fst index pop6.saf.idx pop7.saf.idx -sfs pop6.pop7.ml -fstout pop6.pop7
>>>>>>

chmod +x fst.sh
./fst.sh

#Run for each pop comparison index file
../misc/realSFS fst stats pop1.pop2.fst.idx

#FST stats
#Run for each pop comparison index file
../misc/realSFS fst stats pop0.pop1.fst.idx
# FST.Unweight[nObs:6817706]:0.034252 Fst.Weight:0.121263

../misc/realSFS fst stats pop0.pop2.fst.idx
# FST.Unweight[nObs:6993131]:0.019568 Fst.Weight:0.048224

../misc/realSFS fst stats pop0.pop3.fst.idx
# FST.Unweight[nObs:7018572]:0.024720 Fst.Weight:0.074727

../misc/realSFS fst stats pop0.pop4.fst.idx
#FST.Unweight[nObs:7120329]:0.014838 Fst.Weight:0.035038

../misc/realSFS fst stats pop0.pop5.fst.idx
#FST.Unweight[nObs:6829397]:0.035328 Fst.Weight:0.125949

../misc/realSFS fst stats pop0.pop6.fst.idx
#FST.Unweight[nObs:6993913]:0.016567 Fst.Weight:0.033560

../misc/realSFS fst stats pop0.pop7.fst.idx
#FST.Unweight[nObs:7030323]:0.014457 Fst.Weight:0.038968

../misc/realSFS fst stats pop1.pop2.fst.idx
#FST.Unweight[nObs:7320847]:0.020724 Fst.Weight:0.056960

../misc/realSFS fst stats pop1.pop3.fst.idx
#FST.Unweight[nObs:7462171]:0.015732 Fst.Weight:0.035398

../misc/realSFS fst stats pop1.pop4.fst.idx
#FST.Unweight[nObs:7477669]:0.027870 Fst.Weight:0.113935

../misc/realSFS fst stats pop1.pop5.fst.idx
#FST.Unweight[nObs:7362641]:0.012240 Fst.Weight:0.022469

../misc/realSFS fst stats pop1.pop6.fst.idx
#FST.Unweight[nObs:7255563]:0.032566 Fst.Weight:0.121322

../misc/realSFS fst stats pop1.pop7.fst.idx
#FST.Unweight[nObs:7307584]:0.033242 Fst.Weight:0.134973

../misc/realSFS fst stats pop2.pop3.fst.idx
#FST.Unweight[nObs:7532626]:0.015191 Fst.Weight:0.030638

../misc/realSFS fst stats pop2.pop4.fst.idx
#FST.Unweight[nObs:7723254]:0.015311 Fst.Weight:0.036193

../misc/realSFS fst stats pop2.pop5.fst.idx
#FST.Unweight[nObs:7269843]:0.024419 Fst.Weight:0.077586

../misc/realSFS fst stats pop2.pop6.fst.idx
#FST.Unweight[nObs:7498766]:0.016821 Fst.Weight:0.038018

../misc/realSFS fst stats pop2.pop7.fst.idx
#FST.Unweight[nObs:7595785]:0.016778 Fst.Weight:0.045043

../misc/realSFS fst stats pop3.pop4.fst.idx
#FST.Unweight[nObs:7791134]:0.019739 Fst.Weight:0.066106

../misc/realSFS fst stats pop3.pop5.fst.idx
#FST.Unweight[nObs:7433270]:0.016753 Fst.Weight:0.036125

../misc/realSFS fst stats pop3.pop6.fst.idx
#FST.Unweight[nObs:7576604]:0.021058 Fst.Weight:0.065665

../misc/realSFS fst stats pop3.pop7.fst.idx
#FST.Unweight[nObs:7406283]:0.023794 Fst.Weight:0.082044

../misc/realSFS fst stats pop4.pop5.fst.idx
#FST.Unweight[nObs:7385833]:0.029774 Fst.Weight:0.123724

../misc/realSFS fst stats pop4.pop6.fst.idx
#FST.Unweight[nObs:7714984]:0.011621 Fst.Weight:0.016607

../misc/realSFS fst stats pop4.pop7.fst.idx
#FST.Unweight[nObs:7727503]:0.011578 Fst.Weight:0.021950

../misc/realSFS fst stats pop5.pop6.fst.idx
#FST.Unweight[nObs:7195893]:0.034130 Fst.Weight:0.128870

../misc/realSFS fst stats pop5.pop7.fst.idx
#FST.Unweight[nObs:7185878]:0.036020 Fst.Weight:0.151568

../misc/realSFS fst stats pop6.pop7.fst.idx
#FST.Unweight[nObs:7370535]:0.013362 Fst.Weight:0.021280

# generating per-population SFS
#!/bin/bash
../misc/realSFS pop0.saf.idx >pop0.sfs
../misc/realSFS pop1.saf.idx >pop1.sfs
../misc/realSFS pop2.saf.idx >pop2.sfs
../misc/realSFS pop3.saf.idx >pop3.sfs
../misc/realSFS pop4.saf.idx >pop4.sfs
../misc/realSFS pop5.saf.idx >pop5.sfs
../misc/realSFS pop6.saf.idx >pop6.sfs
../misc/realSFS pop7.saf.idx >pop7.sfs

# generating dadi-like posterior counts based on sfs priors
realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref $GENOME_REF -anc $GENOME_REF >dadiout

# generating dadi-like posterior counts based on sfs priors
nano dadiOut.sh

#!/bin/bash
../misc/realSFS dadi pop0.saf.idx pop1.saf.idx -sfs pop0.sfs -sfs pop1.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout01
../misc/realSFS dadi pop0.saf.idx pop2.saf.idx -sfs pop0.sfs -sfs pop2.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout02
../misc/realSFS dadi pop0.saf.idx pop3.saf.idx -sfs pop0.sfs -sfs pop3.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout03
../misc/realSFS dadi pop0.saf.idx pop4.saf.idx -sfs pop0.sfs -sfs pop4.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout04
../misc/realSFS dadi pop0.saf.idx pop5.saf.idx -sfs pop0.sfs -sfs pop5.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout05
../misc/realSFS dadi pop0.saf.idx pop6.saf.idx -sfs pop0.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout06
../misc/realSFS dadi pop0.saf.idx pop7.saf.idx -sfs pop0.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout07
../misc/realSFS dadi pop1.saf.idx pop2.saf.idx -sfs pop1.sfs -sfs pop2.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout12
../misc/realSFS dadi pop1.saf.idx pop3.saf.idx -sfs pop1.sfs -sfs pop3.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout13
../misc/realSFS dadi pop1.saf.idx pop4.saf.idx -sfs pop1.sfs -sfs pop4.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout14
../misc/realSFS dadi pop1.saf.idx pop5.saf.idx -sfs pop1.sfs -sfs pop5.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout15
../misc/realSFS dadi pop1.saf.idx pop6.saf.idx -sfs pop1.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout16
../misc/realSFS dadi pop1.saf.idx pop7.saf.idx -sfs pop1.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout17
../misc/realSFS dadi pop2.saf.idx pop3.saf.idx -sfs pop2.sfs -sfs pop3.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout23
../misc/realSFS dadi pop2.saf.idx pop4.saf.idx -sfs pop2.sfs -sfs pop4.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout24
../misc/realSFS dadi pop2.saf.idx pop5.saf.idx -sfs pop2.sfs -sfs pop5.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout25
../misc/realSFS dadi pop2.saf.idx pop6.saf.idx -sfs pop2.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout26
../misc/realSFS dadi pop2.saf.idx pop7.saf.idx -sfs pop2.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout27
../misc/realSFS dadi pop3.saf.idx pop4.saf.idx -sfs pop3.sfs -sfs pop4.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout34
../misc/realSFS dadi pop3.saf.idx pop5.saf.idx -sfs pop3.sfs -sfs pop5.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout35
../misc/realSFS dadi pop3.saf.idx pop6.saf.idx -sfs pop3.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout36
../misc/realSFS dadi pop3.saf.idx pop7.saf.idx -sfs pop3.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout37
../misc/realSFS dadi pop4.saf.idx pop5.saf.idx -sfs pop4.sfs -sfs pop5.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout45
../misc/realSFS dadi pop4.saf.idx pop6.saf.idx -sfs pop4.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout46
../misc/realSFS dadi pop4.saf.idx pop7.saf.idx -sfs pop4.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout47
../misc/realSFS dadi pop5.saf.idx pop6.saf.idx -sfs pop5.sfs -sfs pop6.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout56
../misc/realSFS dadi pop5.saf.idx pop7.saf.idx -sfs pop5.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout57
../misc/realSFS dadi pop6.saf.idx pop7.saf.idx -sfs pop6.sfs -sfs pop7.sfs -ref Mcavernosa_July2018.fasta -anc Mcavernosa_July2018.fasta >dadiout67

chmod +x dadiOut.sh
./dadiOut.sh

#pop0=Lower Keys-Meso, 13
#pop1=Lower Keys-Shallow, 30
#pop2=TER-North-Meso, 23
#pop3=TER-North-shallow, 27
#pop4=TER-South-Meso, 37
#pop5=TER-South-shallow, 28
#pop6=Upper-Keys-Meso, 25
#pop7=Upper-Keys-Shallow, 32

# converting to dadi-snp format understood by dadi an Moments:
# I updated the perl script to change the population id column names to match the pairwise comparisons
# numbers after the input file name [4 numbers total] are numbers of individuals sampled per population[ARGV0, ARGV1] followed the population id numbers [ARGV2, ARGV3]
# e.g. "realsfs2dadi.pl dadiout01 13 30 0 1 >01pops_dadi.data" = 13 samples, 30 samples, pop0, pop1

nano realsfs2dadi.sh

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu

realsfs2dadi.pl dadiout01 13 30 0 1 >01pops_dadi.data
realsfs2dadi.pl dadiout02 13 23 0 2 >02pops_dadi.data
realsfs2dadi.pl dadiout03 13 27 0 3 >03pops_dadi.data
realsfs2dadi.pl dadiout04 13 37 0 4 >04pops_dadi.data
realsfs2dadi.pl dadiout05 13 28 0 5 >05pops_dadi.data
realsfs2dadi.pl dadiout06 13 25 0 6 >06pops_dadi.data
realsfs2dadi.pl dadiout07 13 32 0 7 >07pops_dadi.data
realsfs2dadi.pl dadiout12 30 23 1 2 >12pops_dadi.data
realsfs2dadi.pl dadiout13 30 27 1 3 >13pops_dadi.data
realsfs2dadi.pl dadiout14 30 37 1 4 >14pops_dadi.data
realsfs2dadi.pl dadiout15 30 28 1 5 >15pops_dadi.data
realsfs2dadi.pl dadiout16 30 25 1 6 >16pops_dadi.data
realsfs2dadi.pl dadiout17 30 32 1 7 >17pops_dadi.data
realsfs2dadi.pl dadiout23 23 27 2 3 >23pops_dadi.data
realsfs2dadi.pl dadiout24 23 37 2 4 >24pops_dadi.data
realsfs2dadi.pl dadiout25 23 28 2 5 >25pops_dadi.data
realsfs2dadi.pl dadiout26 23 25 2 6 >26pops_dadi.data
realsfs2dadi.pl dadiout27 23 32 2 7 >27pops_dadi.data
realsfs2dadi.pl dadiout34 27 37 3 4 >34pops_dadi.data
realsfs2dadi.pl dadiout35 27 28 3 5 >35pops_dadi.data
realsfs2dadi.pl dadiout36 27 25 3 6 >36pops_dadi.data
realsfs2dadi.pl dadiout37 27 32 3 7 >37pops_dadi.data
realsfs2dadi.pl dadiout45 37 28 4 5 >45pops_dadi.data
realsfs2dadi.pl dadiout46 37 25 4 6 >46pops_dadi.data
realsfs2dadi.pl dadiout47 37 32 4 7 >47pops_dadi.data
realsfs2dadi.pl dadiout56 28 25 5 6 >56pops_dadi.data
realsfs2dadi.pl dadiout57 28 32 5 7 >57pops_dadi.data
realsfs2dadi.pl dadiout67 25 32 6 7 >67pops_dadi.data

###Must be done in a conda environment
#Moments Analysis
module load anaconda/2/2019.03
conda create -n moments python=2.7
source activate moments

cd ~/bin/moments
conda install --file requirements.txt
python import moments
source deactivate

cd ~/2bRAD/floridaKeys/fkSFSAnalysisSfilt

nano 2dAFS.sh

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu
#SBATCH --job-name=2dAFS
#SBATCH --output=2dAFS-%j.out
#SBATCH --error=2dAFS-%j.err

module load anaconda/2/2019.03
source activate moments

#numbers after pops are 2 x number of individuals per pop x 0.9

2dAFS_fold.py 01pops_dadi.data pop0 pop1 23 54
2dAFS_fold.py 02pops_dadi.data pop0 pop2 23 41
2dAFS_fold.py 03pops_dadi.data pop0 pop3 23 48
2dAFS_fold.py 04pops_dadi.data pop0 pop4 23 66
2dAFS_fold.py 05pops_dadi.data pop0 pop5 23 50
2dAFS_fold.py 06pops_dadi.data pop0 pop6 23 45
2dAFS_fold.py 07pops_dadi.data pop0 pop7 23 57
2dAFS_fold.py 12pops_dadi.data pop1 pop2 54 41
2dAFS_fold.py 13pops_dadi.data pop1 pop3 54 48
2dAFS_fold.py 14pops_dadi.data pop1 pop4 54 66
2dAFS_fold.py 15pops_dadi.data pop1 pop5 54 50
2dAFS_fold.py 16pops_dadi.data pop1 pop6 54 45
2dAFS_fold.py 17pops_dadi.data pop1 pop7 54 57
2dAFS_fold.py 23pops_dadi.data pop2 pop3 41 48
2dAFS_fold.py 24pops_dadi.data pop2 pop4 41 66
2dAFS_fold.py 25pops_dadi.data pop2 pop5 41 50
2dAFS_fold.py 26pops_dadi.data pop2 pop6 41 45
2dAFS_fold.py 27pops_dadi.data pop2 pop7 41 57
2dAFS_fold.py 34pops_dadi.data pop3 pop4 48 66
2dAFS_fold.py 35pops_dadi.data pop3 pop5 48 50
2dAFS_fold.py 36pops_dadi.data pop3 pop6 48 45
2dAFS_fold.py 37pops_dadi.data pop3 pop7 48 57
2dAFS_fold.py 45pops_dadi.data pop4 pop5 66 50
2dAFS_fold.py 46pops_dadi.data pop4 pop6 66 45
2dAFS_fold.py 47pops_dadi.data pop4 pop7 66 57
2dAFS_fold.py 56pops_dadi.data pop5 pop6 50 45
2dAFS_fold.py 57pops_dadi.data pop5 pop7 50 57
2dAFS_fold.py 67pops_dadi.data pop6 pop7 45 57

# ------ multimodel inference: fit a diversity of 2-population models, then select the best one based on AIC.
# there are models with a period of exponential growth ("IM" models), models with one, two or three different size and/or migration rate epochs ("SC" models, including models with no migration in some epochs), models with symmetrical migration ("sm" models, in other cases migration is asymmetrical), models with two types of genomic loci ("genomic islands") introgressing at different rates ("i" models), and some fun combinations thereof.
# differences between models are summarized in excel table moments_multimodels.xls

# read about multimodel inference here:
# https://pdfs.semanticscholar.org/a696/9a3b5720162eaa75deec3a607a9746dae95e.pdf

# this HAS to be parallelized - we need to fit ~100 models 5 times to make sure each model converges at its best fit at least once.

# copy the file ".../AFS-analysis-with-moments/multimodel_inference/allmodels_folded" to your working directory

nano allmodels_folded

# running all models on the same data NREPS times, to ensure convergence
# input line: the last four numbers are:
# - projections (2 x 0.9 x number of samples) for in each pop;
# - mutation rate per gamete per generation
# - generation time, in thousand years
ARGS="01pops_dadi.data pop0 pop1 22 54 0.02 0.005"
NREPS=5
>am
for i in `seq 1 $NREPS`;do
#
cat allmodels_folded >>am;
done
NMODELS=`cat am | wc -l`
>args
for i in `seq 1 $NMODELS`; do
echo $ARGS >>args;
done
paste am args -d " " >ama

launcher_creator.py -j ama -n ama -t 2:00:00 -e asturm2017@fau.edu -q shortq7
sbatch ama.slurm

# execute all commands listed in the text file "ama", write all the output into file(s) with extension '.mom'

# collecting results while fixing broken lines
cat *.mom | perl -pe 's/RESULT(.+)(\d)\n/RESULT$1$2/' |perl -pe 's/RESULT(.+)(\d)\n/RESULT$1$2/' |perl -pe 's/RESULT(.+)(\d)\n/RESULT$1$2/' | perl -pe 's/RESULT(.+)([\d\s])\n/RESULT$1$2/' | grep RESULT > mmods.res

# extracting likelihoods and parameter numbers for AIC:
cut -f 2,3,4,5 -d " " mmods.res >likes

# use R script AFS-analysis-with-moments/multimodel_inference/deltaAIC_multimodels.R to find best-fitting model.
# then, using model ID number that the R script will identify as best-fitting model:
# - examine the model's graphic output (*.pdf of actual and modeled SFS, and *.png of the model graph)
# - grep fitted model parameters and their SDs from *.mom files

# the order of parameters are listed in files unfolded_params and folded_params. Typically pop size parameters are first, then times, then migration rates, then the fraction of genomic "islands" (in "i"  models), then percentage of misidentified ancestral states (in unfolded models).

################################################################################
mkdir stacks
cp ~/2bRAD/floridaKeys/mcavANGSDNoClones/fkMcavNoClonesRenamed.vcf.gz ../stacks
logout
 #go to directory where inds2popsNoClonesPopDepth.txt is, ensure if this is saved as a csv to also save as a .txt file to retain the leading zeroes on sample names

scp /Users/student/Documents/GitHub/floridaKeysMcavSnp/data/inds2popsNoClonesPopDepth.txt asturm2017@koko-login.hpc.fau.edu:/home/asturm2017/2bRAD/floridaKeys/stacks

#Log back in and go to working directory

srun populations -V ./fkMcavNoClonesRenamed.vcf.gz -O . -M ./inds2popsNoClonesPopDepth.txt --fstats --fst_correction p_value --p_value_cutoff 0.05 --smooth --bootstrap --bootstrap-reps 9999

################################################################################
mkdir bayescan
cd bayescan
srun cp ../stacks/fkMcavNoClones.vcf.gz . #Note do not use the renamed version, pgdspider has a tough time parsing the samples into pops when you do
srun gunzip fkMcavNoClones.vcf.gz

#scp bspops.txt to koko

echo "############
# VCF Parser questions
PARSER_FORMAT=VCF
# Do you want to include a file with population definitions?
VCF_PARSER_POP_QUESTION=true
# Only input following regions (refSeqName:start:end, multiple regions: whitespace separated):
VCF_PARSER_REGION_QUESTION=
# What is the ploidy of the data?
VCF_PARSER_PLOIDY_QUESTION=DIPLOID
# Only output following individuals (ind1, ind2, ind4, ...):
VCF_PARSER_IND_QUESTION=
# Output genotypes as missing if the read depth of a position for the sample is below:
VCF_PARSER_READ_QUESTION=
# Take most likely genotype if "PL" or "GL" is given in the genotype field?
VCF_PARSER_PL_QUESTION=true
# Do you want to exclude loci with only missing data?
VCF_PARSER_EXC_MISSING_LOCI_QUESTION=false
# Select population definition file:
VCF_PARSER_POP_FILE_QUESTION=./bspops.txt
# Only output SNPs with a phred-scaled quality of at least:
VCF_PARSER_QUAL_QUESTION=
# Do you want to include non-polymorphic SNPs?
VCF_PARSER_MONOMORPHIC_QUESTION=false
# Output genotypes as missing if the phred-scale genotype quality is below:
VCF_PARSER_GTQUAL_QUESTION=
# GESTE / BayeScan Writer questions
WRITER_FORMAT=GESTE_BAYE_SCAN
# Specify which data type should be included in the GESTE / BayeScan file  (GESTE / BayeScan can only analyze one data type per file):
GESTE_BAYE_SCAN_WRITER_DATA_TYPE_QUESTION=SNP
############" >vcf2bayescan.spid

java -Xmx1024m -Xms512m -jar ~/bin/PGDSpider_2.0.7.1/PGDSpider2-cli.jar -inputfile fkMcavNoClones.vcf -outputfile fkMcav.bayescan -spid vcf2bayescan.spid

# launching bayescan (this might take 12-24 hours)
srun bayescan fkMcav.bayescan -threads=20

removeBayescanOutliers.pl bayescan=fkMcav.baye_fst.txt vcf=fkMcavNoClones.vcf FDR=0.1 mode=extract > fkVcfOutliers.vcf

removeBayescanOutliers.pl bayescan=fkMcav.baye_fst.txt vcf=fkMcavNoClones.vcf FDR=0.1 mode=delete > fkVcfNeutral.vcf
################################################################################
#Calculating population parameters no filter on excess heterozygosity
# estimating site frequency likelihoods for each population, also saving allele frequencies (for genome scan)

mkdir angsdPopStats
srun cp ./fkSFSAnalysis/*bam* ./angsdPopStats #copy over all the bam files (clones removed) and the bam lists for each pop. Pop definitions are below

#pop0=Lower Keys-Meso, 13
#pop1=Lower Keys-shallow, 30
#pop2=TER-North-Meso, 23
#pop3=TER-North-shallow, 27
#pop4=TER-South-Meso, 37
#pop5=TER-South-shallow, 28
#pop6=Upper-Keys-Meso, 25
#pop7=Upper-Keys-Shallow, 32


# estimating site frequency likelihoods for each population, also saving allele frequencies (for genome scan)
export GENOME_REF=/home/asturm2017/2bRAD/floridaKeys/referenceGenome/Mcavernosa_July2018.fasta

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -baq 1"
TODO="-doSaf 1 -anc $GENOME_REF -ref $GENOME_REF"

srun angsd -b pop0.bams -minInd 10 -GL 1 -P 1 $TODO $FILTERS -out pop0
srun angsd -b pop1.bams -minInd 23 -GL 1 -P 1 $TODO $FILTERS -out pop1
srun angsd -b pop2.bams -minInd 18 -GL 1 -P 1 $TODO $FILTERS -out pop2
srun angsd -b pop3.bams -minInd 21 -GL 1 -P 1 $TODO $FILTERS -out pop3
srun angsd -b pop4.bams -minInd 28 -GL 1 -P 1 $TODO $FILTERS -out pop4
srun angsd -b pop5.bams -minInd 21 -GL 1 -P 1 $TODO $FILTERS -out pop5
srun angsd -b pop6.bams -minInd 19 -GL 1 -P 1 $TODO $FILTERS -out pop6
srun angsd -b pop7.bams -minInd 24 -GL 1 -P 1 $TODO $FILTERS -out pop7

# generating per-population SFS
nano sfs

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu
#SBATCH --job-name=sfs
#SBATCH --output=sfs-%j.out
#SBATCH --error=sfs-%j.err

realSFS pop0.saf.idx >pop0.sfs
realSFS pop1.saf.idx >pop1.sfs
realSFS pop2.saf.idx >pop2.sfs
realSFS pop3.saf.idx >pop3.sfs
realSFS pop4.saf.idx >pop4.sfs
realSFS pop5.saf.idx >pop5.sfs
realSFS pop6.saf.idx >pop6.sfs
realSFS pop7.saf.idx >pop7.sfs

##Producing thetas

nano theta

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu
#SBATCH --job-name=theta

realSFS saf2theta pop0.saf.idx -sfs pop0.sfs -outname pop0
thetaStat do_stat pop0.thetas.idx
realSFS saf2theta pop1.saf.idx -sfs pop1.sfs -outname pop1
thetaStat do_stat pop1.thetas.idx
realSFS saf2theta pop2.saf.idx -sfs pop2.sfs -outname pop2
thetaStat do_stat pop2.thetas.idx
realSFS saf2theta pop3.saf.idx -sfs pop3.sfs -outname pop3
thetaStat do_stat pop3.thetas.idx
realSFS saf2theta pop4.saf.idx -sfs pop4.sfs -outname pop4
thetaStat do_stat pop4.thetas.idx
realSFS saf2theta pop5.saf.idx -sfs pop5.sfs -outname pop5
thetaStat do_stat pop5.thetas.idx
realSFS saf2theta pop6.saf.idx -sfs pop6.sfs -outname pop6
thetaStat do_stat pop6.thetas.idx
realSFS saf2theta pop7.saf.idx -sfs pop7.sfs -outname pop7
thetaStat do_stat pop7.thetas.idx

##Producing Heterozygosity
nano hetero

#!/bin/sh
#SBATCH --partition shortq7
#SBATCH --nodes 1
#SBATCH --exclusive
#SBATCH --mail-type=all
#SBATCH --mail-user=asturm2017@fau.edu
#SBATCH --job-name=hetero

realSFS pop0.saf.idx >pop0.ml
realSFS pop1.saf.idx >pop1.ml
realSFS pop2.saf.idx >pop2.ml
realSFS pop3.saf.idx >pop3.ml
realSFS pop4.saf.idx >pop4.ml
realSFS pop5.saf.idx >pop5.ml
realSFS pop6.saf.idx >pop6.ml
realSFS pop7.saf.idx >pop7.ml

#####################NEW SFS Analysis
#Obtaining bootstrapped SFS with ANGSD
#Here we obtain 100 series of 6 bootstrap replicates. For each of the series, we discard the first replicate (it is just the original data, according to ANGSD pundit Nate Pope) and average the remaining 5. This procedure is called "bagging" and is designed to mitigate the noise that ANGSD-derived SFS often show, especially for small datasets (i.e. RAD-seq). The resulting 100 "bagged" datasets are going to be our bootstrap replicates.

#Let's assume we have two populations, p1 and p2, each with 10 sequenced individuals, and we have two text files, p1.bams and p2.bams, listing *.bam files for each population. First we need to collect sites (variable and invariable!) that pass our filters in both populations:

mkdir sfsAnalysis
cd sfsAnalysis *copy over bams files without clones

#pop0=Lower Keys-Meso, 13
#pop1=Lower Keys-shallow, 30
#pop2=TER-North-Meso, 23
#pop3=TER-North-shallow, 27
#pop4=TER-South-Meso, 37
#pop5=TER-South-shallow, 28
#pop6=Upper-Keys-Meso, 25
#pop7=Upper-Keys-Shallow, 32

GRate=0.75 # genotyping rate filter - a site must be genotyped in this fraction of all samples.
cat pop0.bams pop1.bams > p01.bams

FILTERS='-uniqueOnly 1 -skipTriallelic 1 -minMapQ 30 -minQ 30 -doHWE 1 -maxHetFreq 0.5 -sb_pval 1e-5 -hetbias_pval 1e-5 -minInd $MI'
TODO='-doMajorMinor 1 -doMaf 1 -dosnpstat 1 -doPost 2'
echo 'export NIND=`cat p01.bams | wc -l`; export MI=`echo "($NIND*$GRate+0.5)/1" | bc`' >calc
source calc && angsd -b p01.bams -GL 1 -P 4 $FILTERS $TODO -out p01 &

# wait a while...

zcat p01.mafs.gz | cut -f 1,2 | tail -n +2 > goodsites
angsd sites index goodsites
